{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle_file_path = \"/content/drive/MyDrive/chunks.pkl\"\n",
        "with open(pickle_file_path, \"rb\") as f:\n",
        "    loaded_chunks = pickle.load(f)\n",
        "print(\"Chunks loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "9olWvgAOHIvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "\n",
        "embeddings_model = HuggingFaceEmbeddings(model_name=model_name)"
      ],
      "metadata": {
        "id": "DuuNo5ilGy91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n",
        "!pip install langchain-chroma"
      ],
      "metadata": {
        "id": "k3mNiE7UG75S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6QSfPMqyT89"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Initialize the database connection\n",
        "# If database exist, it will connect with the collection_name and persist_directory\n",
        "# Otherwise a new collection will be created\n",
        "db = Chroma(collection_name=\"vector_database\",\n",
        "            embedding_function=embedding_model,\n",
        "            persist_directory=\"/content/drive/MyDrive/chroma_db_\")\n",
        "\n",
        "# We can check the already existing values\n",
        "print(len(db.get()[\"ids\"]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "pRKkrr-9HO4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b-Vddt7SIZh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Initialize a Chat Prompt Template\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Answer the question based only on the following context:\n",
        "{context}\n",
        "Answer the question based on the above context: {question}.\n",
        "Provide a detailed answer.\n",
        "Don’t justify your answers.\n",
        "Don’t give information not mentioned in the CONTEXT INFORMATION.\n",
        "Do not say \"according to the context\" or \"mentioned in the context\" or similar.\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)"
      ],
      "metadata": {
        "id": "KvaBUnCWHXgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r9T4TXvsIB-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other parameters as needed...\n",
        ")"
      ],
      "metadata": {
        "id": "DWN9QhThHZK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "qQmrFF6EHo3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Define a RAG Chain\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# The chain is defined as:\n",
        "#   { \"context\": retriever | format_docs, \"question\": RunnablePassthrough() }\n",
        "#   piped through the prompt template, then through the chat model (llm), then the output parser.\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt_template\n",
        "    | llm\n",
        "    | parser\n",
        ")"
      ],
      "metadata": {
        "id": "LspVS4LVHsw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'Who is Rachem?'\n",
        "\n",
        "rag_chain.invoke(query)"
      ],
      "metadata": {
        "id": "iPsJZZRTH2Kd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}